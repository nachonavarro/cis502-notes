%!TEX root = ../main.tex

\section{Vector Spaces Review}

\begin{definition}
    $V$ is vector space over $\mathbb{R}^n$ if
    \begin{itemize}
        \item For any $v_1, v_2 \in V \implies v_1 + v_2 \in V$.
        \item For any $\alpha \in \mathbb{R}$ and $v \in V$ then
        $\alpha v \in V$.
    \end{itemize}
\end{definition}

\begin{definition}
    Given a finite set of vectors $v_1, ..., v_k$, the span $S(v_1,
    ..., v_k)$ is the set of all linearly combinations, i.e.
    $$
    S(v_1, ..., v_k) = \{\alpha_1v_1 + ... + \alpha_kv_k : \alpha_i
    \in \mathbb{R}\}
    $$
\end{definition}

\begin{definition}
    A set of vectors $v_1, ..., v_k$ is linearly dependent if there
    exist some non-trivial $\alpha_1, ..., \alpha_k$ with
    $$
    \alpha_1v_1 + ... + \alpha_kv_k = 0
    $$
\end{definition}

\begin{lemma}
    The span is a vector space.
\end{lemma}

\begin{definition}
    A set of vectors is said to be linearly independent if they are
    not linearly dependent.
\end{definition}

\begin{definition}
    Let $v_1, ..., v_k$ be linearly independent and suppose their
    span is $V$, then $v_1, ..., v_k$ form a basis for $V$.
\end{definition}

\begin{lemma}
    If $v_1, ..., v_k$ is a basis for $V$ and $u_1, ..., u_k$ is
    another basis for $V$, then $m = k$.
\end{lemma}

\begin{proof}
    (Sketch). Suppose for a contradiction that $m > k$. Then we have
    \begin{align*}
        u_1 &= \alpha_{11}v_1 + ... + \alpha_{1k}v_k \\
        u_2 &= \alpha_{21}v_1 + ... + \alpha_{2k}v_k \\
        \vdots \\
        u_m &= \alpha_{m1}v_1 + ... + \alpha_{mk}v_k
    \end{align*}
    We need to show that $u_1, ..., u_m$ is linearly dependent. To
    show this, we need to find some non-trivial $x_1, ..., x_m$ such
    that $\sum_i x_iu_i = 0$. So suppose $\sum_i x_iu_i = 0$.
    Substitute each $u_i$ with the equations above. Collect terms, and
    you'll get a system of equations with $k$ equations and $m$
    unknowns, which leads to infinitely many solutions since $m > k$
    by assumption. This leads to a contradiction.
\end{proof}

\section{More Greedy Algorithms}

\subsection{Basis of Maximal Weight}

Problem: Suppose you are Google and you assign a vector to each
possible
document. When a user searches for a term, you want to collect some
vectors that are indicative of that search term. You also want to be
diverse, e.g., if a user searches for "jaguar" you want to return
some documents with cars and others with the animal. We could model
this by returning vectors that are linearly independent.

More abstractly, given vectors $v_1, ..., v_n$ with weights $w_1, ...,
w_n$, we need to return a basis (note there can be many!) for the
space spanned by these vectors of maximum total weight.

\subsubsection{Greedy Approach}

Sort the vectors by decreasing order of weights $v_1, ..., v_n$. Let
$S$ be a set of vectors. For each vector $v_i$ in this order, if $S
\cup \{v_i\}$ is linearly independent, then assign $S = S \cup 
\{v_i\}$.

\subsubsection{Correctness}

By contradiction. Suppose the greedy approach returns vectors $G = 
\{v_{i_1}, ..., v_{i_k}\}$,
and suppose there is an optimal solution with better total weight that
returns vectors
$O = \{v_{j_1}, ..., v_{j_k}\}$.

\textbf{Note}: the next part is the start of lecture 10, but I'll put it 
here for continuity.

\begin{remark}
    Suppose $A$ and $B$ are finite subsets of linearly independent
    vectors in $\mathbb{R}^d$ and suppose $|A|Â < |B|$. Then, using
    the exchange property, there exist some vector $b \in B$ such that
    $A \cup \{b\}$ is linearly independent. Indeed, there must be a
    vector in $B$ that lies outside the space spanned by $A$ because
    of the dimensions of $\langle B \rangle$ and $\langle A \rangle$.
\end{remark}

Let $v_{i_l}$ be the first vector chosen by the greedy algorithm that
is not in $O$. Using our remark, we have that 
$A = \{v_{i_1}, ..., v_{i_l}\}$, and $B = O$.

\begin{itemize}
    \item Case 1: $l = k$. But then $G$ is at least as good
    as $O$, which is a contradiction.
    \item Case 2: $ l < k$, i.e., $|A| < |B|$. Apply our remark.
    By the exchange property repeatedly, we can keep adding vectors
    from $B$ to $A$, preserving linear independence of $A$ until
    $|A| = |B|$. At the end, $A$ has $v_{i_l}$ instead of some
    vector in $O$ that has weight less than $v_{i_l}$. Therefore,
    $w(A) \geq w(O)$. Note that even if $w(A) = w(O)$, $A$ agrres more
    with our greedy solution than $O$, so by induction greedy must be
    as good as $O$.
\end{itemize}

\begin{definition}
    (Hereditary property). If $R \subset S$ and $S$ is a linearly
    independent set of vectors, then $R$ is also linearly independent.
\end{definition}

The greedy algorithm for finding a maximum weight basis works because
vectors have the exchange and hereditary properties.





